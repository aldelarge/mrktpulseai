from scraper import scrape_yahoo_finance
from scraper import scrape_yahoo_indices
from scraper import scrape_cnbc_news
from scraper import scrape_marketwatch
from scraper import scrape_yahoo_sectors
from sentiment_analysis import analyze_daily_sentiment_gpt, analyze_weekly_sentiment_gpt
from polygon_api import get_index_snapshot
from polygon_api import format_etf_data
from send_email import send_email
from markdown import format_market_summary
from stock_news_api import fetch_top_headlines, format_market_analysis
import json
import os
from datetime import datetime
import pytz
import asyncio
from webapp import create_app, db
from webapp.models import User, StockData
from daily_data import fetch_and_summarize_stock_news
from sqlalchemy.orm import joinedload




WEEKLY_DATA_FILE = "weekly_data.json"
WEEKLY_REPORTS_FILE = "weekly_reports.json"

app = create_app()

current_day = datetime.now().weekday()

def is_weekday():
    eastern = pytz.timezone("US/Eastern")
    now = datetime.now(eastern)
    return now.weekday() < 5  # 0 = Monday, 4 = Friday

def format_sector_performance(sectors):
    formatted = "\n".join([f"{sector['sector']}: {sector['percent_change']}" for sector in sectors])
    return f"Sector Performance:\n{formatted}"
# For storing the weekly report
def save_weekly_report(report):
    """Save the weekly report generated by GPT-4 for future reference."""
    weekly_report = {
        "date": datetime.now().strftime("%Y-%m-%d"),  # Add the current date for easy identification
        "report": report
    }

    # Load existing reports or initialize an empty list
    if os.path.exists(WEEKLY_REPORTS_FILE):
        with open(WEEKLY_REPORTS_FILE, "r") as file:
            weekly_reports = json.load(file)
    else:
        weekly_reports = []

    weekly_reports.append(weekly_report)

    with open(WEEKLY_REPORTS_FILE, "w") as file:
        json.dump(weekly_reports, file, indent=4)

    print("ðŸ“Œ Weekly market report saved.")

# Function to handle daily tasks (scraping, sentiment analysis, saving data)
async def daily_tasks():
    today = datetime.now().strftime("%Y-%m-%d")

    # Scraping data
    # news1 = scrape_yahoo_finance()
    # news2 = scrape_cnbc_news()
    # news3 = scrape_marketwatch()
    news = fetch_top_headlines(limit=100)
    sectors_data = scrape_yahoo_sectors()
    sector_summary = format_sector_performance(sectors_data)
    snapshot = get_index_snapshot()
    indices = format_etf_data(snapshot)
    # print(indices)
    # print(sector_summary)

    # Validate that news lists are actually lists
    # if not isinstance(news1, list) or not isinstance(news2, list):
    #     print("Error: One of the news sources did not return a valid list.")
    #     return  
    # if not news1:
    #     print("Warning: Yahoo Finance scraper returned no headlines.")
    # if not news2:
    #     print("Warning: CNBC scraper returned no headlines.")
    # if not news3:
    #     print("Warning: Marketwatch scraper returned no headlines.")
    if not indices:
        print("Warning: Indices data scraper returned no data.")
    if not sectors_data:
        print("Warning: Sectors data scraper returned no data.")

    # Extract headlines from the scraped data
    # yahoo_headlines = [article["headline"] for article in news1 if isinstance(article, dict)]
    # cnbc_headlines = [article["headline"] for article in news2 if isinstance(article, dict)]
    # marketwatch_headlines = [article["headline"] for article in news3 if isinstance(article, dict)]

    # Combine all headlines into one list
    headlines = format_market_analysis(news)
    # print(headlines)
    if headlines:
        sentiment_summary = analyze_daily_sentiment_gpt(headlines, indices, sector_summary)
        # print(f"\n {sentiment_summary}")

        # Save today's data
        # save_daily_newsletter(today, indices, sector_summary, sentiment_summary)
        
        # key_points = extract_key_points(sentiment_summary)
        # store_summary_key_points(key_points)

        fetch_and_summarize_stock_news()  # This ensures stock summaries are up-to-date.

        return sentiment_summary
    else:
        print("No news articles found.")

# Function to handle weekly tasks (reading weekly data, scraping weekend news, generating weekly summary)
async def weekly_tasks():
    today = datetime.now().strftime("%Y-%m-%d")
    WEEKLY_DATA_FILE = "weekly_data.json"

    # Get the weekly data from the JSON file
    if os.path.exists(WEEKLY_DATA_FILE):
        with open(WEEKLY_DATA_FILE, "r") as file:
            weekly_data = json.load(file)
    else:
        print("No weekly data found.")
        return

    # Scrape the weekend news (Friday, Saturday, Sunday)
    weekend_news1 = scrape_yahoo_finance()
    weekend_news2 = scrape_cnbc_news()
    weekend_news3 = scrape_marketwatch()

    # Extract headlines for the weekend news
    weekend_yahoo_headlines = [article["headline"] for article in weekend_news1 if isinstance(article, dict)]
    weekend_cnbc_headlines = [article["headline"] for article in weekend_news2 if isinstance(article, dict)]
    weekend_marketwatch_headlines = [article["headline"] for article in weekend_news3 if isinstance(article, dict)]

    # Combine the weekend headlines into one list
    weekend_headlines = weekend_yahoo_headlines + weekend_cnbc_headlines + weekend_marketwatch_headlines

    if weekend_headlines:
        sentiment_summary = analyze_weekly_sentiment_gpt(weekend_headlines, weekly_data)
        print(f"\n {sentiment_summary}")

        # Save the weekly report
        save_weekly_report(sentiment_summary)
        return sentiment_summary
    else:
        print("No weekend news found.")

async def main():
    print("Starting main function...")
    
    daily_market_update = await daily_tasks()

    print(f"Market Update: {daily_market_update}") 

    with app.app_context():
        users = User.query.options(joinedload(User.saved_stocks)).all()

        print("Sending personalized emails....")

        for user in users:
            if user.subscription_status != "active":
                continue

            # âœ… Fetch user's tracked stocks
            tracked_stocks = [row.stock_symbol for row in user.saved_stocks]

            # âœ… Retrieve summaries for the user's stocks
            user_stock_summaries = []
            for symbol in tracked_stocks:
                stock_entry = StockData.query.filter_by(symbol=symbol).first()
                if stock_entry and stock_entry.summary_text:
                    user_stock_summaries.append(f"{symbol}: {stock_entry.summary_text}")

            # âœ… Format the entire email using one function
            styled_email = format_market_summary(daily_market_update, user_stock_summaries)

            # âœ… Send the email
            try:
                result = send_email("ðŸ“Š Your Daily Market Update", styled_email, user.email)
                print(f"âœ… Email sent to {user.email}")
            except Exception as e:
                print(f"âŒ Error sending to {user.email}: {e}")

    print("âœ… All emails sent!")

if __name__ == "__main__":
    if is_weekday():
        asyncio.run(main())
    else:
        print("ðŸ›Œ It's the weekend. Skipping weekday market email.")